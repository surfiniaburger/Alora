{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "izM0JLflUBmR"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pQ0kDzXUBmS"
      },
      "source": [
        "# SAM 3 Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbmryCSTUBmS"
      },
      "source": [
        "This notebook shows an example of how an MLLM can use SAM 3 as a tool, i.e., \"SAM 3 Agent\", to segment more complex text queries such as \"the leftmost child wearing blue vest\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izGZKc9eUBmS"
      },
      "source": [
        "## Env Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cbhWzCCUBmT"
      },
      "source": [
        "First install `sam3` in your environment using the [installation instructions](https://github.com/facebookresearch/sam3?tab=readme-ov-file#installation) in the repository."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!conda create -n sam3 python=3.12\n",
        "!conda deactivate\n",
        "!conda activate sam3"
      ],
      "metadata": {
        "id": "4KC9oiykUolz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_54_hXeNUt1B",
        "outputId": "2787440d-f93a-4ce9-d293-fe595e6263c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Collecting torch==2.7.0\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torch-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.80)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cudnn-cu12/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.5.4.2)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-cusparselt-cu12/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.0)\n",
            "  Downloading https://pypi.nvidia.com/nvidia-nccl-cu12/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (1.11.1.6)\n",
            "Collecting triton==3.3.0 (from torch==2.7.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.24.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.23.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.9.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.8.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.1%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.7.0) (3.0.3)\n",
            "Downloading https://download.pytorch.org/whl/cu126/torch-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (866.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.8/866.8 MB\u001b[0m \u001b[31m765.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchvision-0.22.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu126/torchaudio-2.7.0%2Bcu126-cp312-cp312-manylinux_2_28_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nccl-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cu126\n",
            "    Uninstalling torchvision-0.24.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.24.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.9.0+cu126\n",
            "    Uninstalling torchaudio-2.9.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "Successfully installed nvidia-cudnn-cu12-9.5.1.17 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 torch-2.7.0+cu126 torchaudio-2.7.0+cu126 torchvision-0.22.0+cu126 triton-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzNSBDl4V7df",
        "outputId": "45de77a0-21cf-48e7-d654-331c74427ed0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch==2.7.0 in /usr/local/lib/python3.12/dist-packages (2.7.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.22.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.7.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.7.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.7.0) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Remove existing sam3 directory if it exists to ensure a clean clone\n",
        "!rm -rf sam3\n",
        "!git clone https://github.com/facebookresearch/sam3.git"
      ],
      "metadata": {
        "id": "KjrHa5wgU1Sm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fileinput\n",
        "import os\n",
        "\n",
        "sam3_root_dir = \"/content/sam3\" # Using absolute path, as SAM3_ROOT might not be defined yet\n",
        "\n",
        "pyproject_path = os.path.join(sam3_root_dir, \"pyproject.toml\")\n",
        "temp_file_path = pyproject_path + \".tmp\"\n",
        "\n",
        "# Check if pyproject.toml exists before trying to modify\n",
        "if os.path.exists(pyproject_path):\n",
        "    print(f\"Attempting to modify {pyproject_path} to remove strict numpy version pinning.\")\n",
        "    modified_lines = []\n",
        "    with open(pyproject_path, 'r') as infile:\n",
        "        for line in infile:\n",
        "            if 'numpy==' in line:\n",
        "                # Replace the pinned version, allowing pip to use a compatible version\n",
        "                modified_line = line.replace('numpy==1.26', 'numpy')\n",
        "                modified_lines.append(modified_line)\n",
        "            else:\n",
        "                modified_lines.append(line)\n",
        "\n",
        "    with open(temp_file_path, 'w') as outfile:\n",
        "        outfile.writelines(modified_lines)\n",
        "\n",
        "    os.replace(temp_file_path, pyproject_path)\n",
        "    print(f\"Modified {pyproject_path}.\")\n",
        "else:\n",
        "    print(f\"Warning: {pyproject_path} not found. Skipping pyproject.toml modification.\")\n",
        "\n",
        "# Original content of DPMUoaL8VrkE\n",
        "!cd /content/sam3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPMUoaL8VrkE",
        "outputId": "aac5cced-0532-4247-829c-9a31e4687fcb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to modify /content/sam3/pyproject.toml to remove strict numpy version pinning.\n",
            "Modified /content/sam3/pyproject.toml.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "\n",
        "# Ensure the current directory is sam3 for pip install\n",
        "!cd sam3 && pip install -e ."
      ],
      "metadata": {
        "id": "MJBW-9-aVuVJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# For running example notebooks\n",
        "!cd sam3 && pip install -e \".[notebooks]\"\n",
        "\n",
        "# For development\n",
        "!cd sam3 && pip install -e \".[train,dev]\""
      ],
      "metadata": {
        "id": "uGicETvyU5zV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4yc5wp6TUBmT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "# turn on tfloat32 for Ampere GPUs\n",
        "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# use bfloat16 for the entire notebook. If your card doesn't support it, try float16 instead\n",
        "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "# inference mode for the whole notebook. Disable if you need gradients\n",
        "torch.inference_mode().__enter__()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%capture\n",
        "!hf login"
      ],
      "metadata": {
        "id": "IZokhs6lUGy2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5pKvXtYRUBmT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# SAM3_ROOT should point to the cloned sam3 repository\n",
        "SAM3_ROOT = \"/content/sam3\"\n",
        "os.chdir(SAM3_ROOT)\n",
        "\n",
        "# setup GPU to use -  A single GPU is good with the purpose of this demo\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "_ = os.system(\"nvidia-smi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG1_ravYUBmT"
      },
      "source": [
        "## Build SAM3 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "a58249780a054b65834b14cd4e5293ac",
            "782eb73fbc64434fb7841da99830abc6",
            "599c40e16a2e400a938b98aca2b101a1",
            "342af68877a94b7a9978c4c1259dcba6",
            "1427a8e66c2c4634aee8706ef1aa1edd",
            "86a7545da1ca4deab4d9e2ac5e227607",
            "e7fac4fd0c6e4a74aac104b639b123fe",
            "46eaf90f007f443eb91c3d4161de193b",
            "829bef6440dd42f89905ac6f484401a2",
            "b85fe51fe50d4928854f3fd78ebe31d3",
            "40fceb5cee4c4901bc0e09b40ffad867",
            "88fdf57c6bf2445c9f823eccb50723bb",
            "669c4d70c1214798b1fbf3c9582aba45",
            "0457fc948c944d4fb0aacbdf18538366",
            "72ee877b31214dd99ef3d8f43f1b54b2",
            "e823122c6edd448ea4a95bea13e247c6",
            "eca908d9bb7649be96d0824ca99c3a8f",
            "4ccc7efc183d43afb75fe4071cf301ff",
            "6980adb692094bdd8673048be5b40841",
            "aea689d0b30f40c887465fc6a077ea3e",
            "f840e83ae743468481cc6ac785e1cfd9",
            "90fdb0d6f1f9430a87030789b7a6652b"
          ]
        },
        "id": "9-A5iKEqUBmT",
        "outputId": "e3eeb2fe-7e02-4574-f18f-bf945d712bdb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/25.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a58249780a054b65834b14cd4e5293ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sam3.pt:   0%|          | 0.00/3.45G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88fdf57c6bf2445c9f823eccb50723bb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# SAM3_ROOT is expected to be '/content/sam3' from previous steps\n",
        "SAM3_ROOT = \"/content/sam3\"\n",
        "if SAM3_ROOT not in sys.path:\n",
        "    sys.path.insert(0, SAM3_ROOT)\n",
        "\n",
        "import sam3\n",
        "from sam3 import build_sam3_image_model\n",
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "\n",
        "sam3_root = os.path.dirname(sam3.__file__)\n",
        "bpe_path = f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
        "model = build_sam3_image_model(bpe_path=bpe_path)\n",
        "processor = Sam3Processor(model, confidence_threshold=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjNQXr7rUBmT"
      },
      "source": [
        "## LLM Setup\n",
        "\n",
        "Config which MLLM to use, it can either be a model served by vLLM that you launch from your own machine or a model is served via external API. If you want to using a vLLM model, we also provided insturctions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YbvecvshUBmT"
      },
      "outputs": [],
      "source": [
        "LLM_CONFIGS = {\n",
        "    # models served via external APIs\n",
        "    \"gemini-3-pro-preview\": {\n",
        "        \"provider\": \"gemini\",\n",
        "        \"model\": \"gemini-3-pro-preview\",\n",
        "        \"base_url\": \"\", # Not needed for Gemini API\n",
        "    },\n",
        "}\n",
        "\n",
        "model = \"gemini-3-pro-preview\"\n",
        "# Note: GEMINI_API_KEY should be stored securely in Colab Secrets\n",
        "LLM_API_KEY = \"DUMMY_API_KEY\" # This will be replaced by actual key from Colab secrets\n",
        "\n",
        "llm_config = LLM_CONFIGS[model]\n",
        "llm_config[\"api_key\"] = LLM_API_KEY # This will be overwritten by actual key in setup_gemini_api_key cell\n",
        "llm_config[\"name\"] = model\n",
        "\n",
        "# Setup API endpoint (not strictly needed for Gemini with direct SDK call, but keeping structure)\n",
        "if llm_config[\"provider\"] == \"vllm\":\n",
        "    LLM_SERVER_URL = \"http://0.0.0.0:8001/v1\"  # replace this with your vLLM server address as needed\n",
        "else:\n",
        "    LLM_SERVER_URL = llm_config[\"base_url\"] if \"base_url\" in llm_config and llm_config[\"base_url\"] else \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8_oPLDiUBmT"
      },
      "source": [
        "### Setup vLLM server\n",
        "This step is only required if you are using a model served by vLLM, skip this step if you are calling LLM using an API like Gemini and GPT.\n",
        "\n",
        "* Install vLLM (in a separate conda env from SAM 3 to avoid dependency conflicts).\n",
        "  ```bash\n",
        "    conda create -n vllm python=3.12\n",
        "    pip install vllm --extra-index-url https://download.pytorch.org/whl/cu128\n",
        "  ```\n",
        "* Start vLLM server on the same machine of this notebook\n",
        "  ```bash\n",
        "    # qwen 3 VL 8B thinking\n",
        "    vllm serve Qwen/Qwen3-VL-8B-Thinking --tensor-parallel-size 4 --allowed-local-media-path / --enforce-eager --port 8001\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf2x_Bt6UBmT"
      },
      "source": [
        "## Run SAM3 Agent Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfh_RkCNUBmT",
        "outputId": "cf432ba0-f497-44bb-ed00-0772b0cb2b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.12.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.5)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
            "Gemini API key configured from Colab secrets and environment.\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "from IPython.display import display, Image\n",
        "from sam3.agent.client_llm import send_generate_request as send_generate_request_orig\n",
        "from sam3.agent.client_sam3 import call_sam_service as call_sam_service_orig\n",
        "from sam3.agent.inference import run_single_image_inference\n",
        "\n",
        "# Ensure google-generativeai is up to date\n",
        "!pip install --upgrade google-generativeai\n",
        "\n",
        "# Install packages as per user's suggestion\n",
        "!pip install -U -q \"google\"\n",
        "!pip install -U -q \"google.genai\"\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import os # Import os for environment variable\n",
        "\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    # Set environment variable as suggested by user's snippet\n",
        "    os.environ[\"GEMINI_API_KEY\"] = GOOGLE_API_KEY\n",
        "    print(\"Gemini API key configured from Colab secrets and environment.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"WARNING: GOOGLE_API_KEY not found in Colab secrets. Please add it to secrets manager.\")\n",
        "    print(\"Using a DUMMY_API_KEY, requests to Gemini will likely fail.\")\n",
        "    GOOGLE_API_KEY = \"DUMMY_API_KEY\"\n",
        "    os.environ[\"GEMINI_API_KEY\"] = GOOGLE_API_KEY # Still set dummy for consistency\n",
        "\n",
        "# Adjust send_generate_request to use Gemini SDK directly if provider is 'gemini'\n",
        "def send_generate_request_gemini(messages, model_name, api_key, server_url):\n",
        "    llm_model = genai.GenerativeModel(model_name)\n",
        "\n",
        "    gemini_formatted_messages = []\n",
        "    for msg in messages:\n",
        "        role = 'model' if msg['role'] == 'assistant' else msg['role']\n",
        "        gemini_formatted_messages.append(\n",
        "            {'role': role, 'parts': [{'text': msg['content']}]}\n",
        "        )\n",
        "\n",
        "    # Explicitly define GenerationConfig with a temperature\n",
        "    generation_config = genai.GenerationConfig(\n",
        "        temperature=0.7,  # Default creative temperature\n",
        "        max_output_tokens=2048 # A common maximum token limit\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = llm_model.generate_content(gemini_formatted_messages, generation_config=generation_config)\n",
        "    except Exception as e:\n",
        "        print(f\"DEBUG: Error during generate_content call: {e}\")\n",
        "        # Return a string with the error message as expected by agent_core.py\n",
        "        return f\"ERROR: Gemini API call failed: {e}\"\n",
        "\n",
        "    generated_text = \"\"\n",
        "    if hasattr(response, 'text'):\n",
        "        generated_text = response.text\n",
        "    elif hasattr(response, 'candidates') and response.candidates:\n",
        "        # Get the first candidate's content\n",
        "        candidate_content = response.candidates[0].content\n",
        "        if hasattr(candidate_content, 'parts') and candidate_content.parts:\n",
        "            # Concatenate text from all parts of the first candidate\n",
        "            generated_text = \"\".join(part.text for part in candidate_content.parts if hasattr(part, 'text'))\n",
        "\n",
        "        # Check for safety reasons if no text was generated or if a block occurred\n",
        "        if not generated_text:\n",
        "            if hasattr(response.candidates[0], 'finish_reason') and response.candidates[0].finish_reason == 'SAFETY':\n",
        "                generated_text = \"ERROR: Gemini response was blocked due to safety concerns.\"\n",
        "            elif hasattr(response.candidates[0], 'finish_reason') and response.candidates[0].finish_reason == 'RECITATION':\n",
        "                generated_text = \"ERROR: Gemini response was blocked due to recitation.\"\n",
        "            elif hasattr(response, 'prompt_feedback') and response.prompt_feedback and hasattr(response.prompt_feedback, 'block_reason'):\n",
        "                generated_text = f\"ERROR: Prompt was blocked: {response.prompt_feedback.block_reason}\"\n",
        "\n",
        "\n",
        "    if not generated_text:\n",
        "        # Fallback if no text could be extracted or other unexpected response\n",
        "        print(f\"DEBUG: Gemini response did not contain accessible text. Full response object: {response}\")\n",
        "        generated_text = f\"ERROR: Could not extract text from Gemini response. Raw response: {str(response)}\"\n",
        "\n",
        "    # The SAM3 agent expects a plain string, not a dictionary wrapper\n",
        "    return generated_text\n",
        "\n",
        "# Overwrite send_generate_request if using Gemini\n",
        "if llm_config[\"provider\"] == \"gemini\":\n",
        "    send_generate_request = partial(send_generate_request_gemini, model_name=llm_config[\"model\"], api_key=llm_config[\"api_key\"], server_url=LLM_SERVER_URL)\n",
        "else:\n",
        "    send_generate_request = partial(send_generate_request_orig, server_url=LLM_SERVER_URL, model=llm_config[\"model\"], api_key=llm_config[\"api_key\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "output": {
          "id": 689664053567678,
          "loadingStatus": "loaded"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        },
        "id": "sKC1v43nUBmT",
        "outputId": "d1067343-f31e-4496-9aae-e155d236e4d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------ Starting SAM 3 Agent Session... ------------------------------ \n",
            "> Text prompt: the leftmost child wearing blue vest\n",
            "> Image path: /content/sam3/assets/images/test_image.jpg\n",
            "\n",
            "\n",
            "\n",
            "------------------------------ Round 1------------------------------\n",
            "\n",
            "\n",
            "\n",
            "DEBUG: Error during generate_content call: 'ProtoType' object has no attribute 'DESCRIPTOR'\n",
            "\n",
            ">>> MLLM Response [start]\n",
            "ERROR: Gemini API call failed: 'ProtoType' object has no attribute 'DESCRIPTOR'\n",
            "<<< MLLM Response [end]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid JSON in tool call: ERROR: Gemini API call failed: 'ProtoType' object has no attribute 'DESCRIPTOR'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/content/sam3/sam3/agent/agent_core.py\u001b[0m in \u001b[0;36magent_inference\u001b[0;34m(img_path, initial_text_prompt, debug, send_generate_request, call_sam_service, max_generations, output_dir)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mtool_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_call_json_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3136627262.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcall_sam_service\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_sam_service_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msam3_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m output_image_path = run_single_image_inference(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msend_generate_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_sam_service\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"agent_output\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/sam3/sam3/agent/inference.py\u001b[0m in \u001b[0;36mrun_single_image_inference\u001b[0;34m(image_path, text_prompt, llm_config, send_generate_request, call_sam_service, output_dir, debug)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'-'*30} Starting SAM 3 Agent Session... {'-'*30} \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     agent_history, final_output_dict, rendered_final_output = agent_inference(\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtext_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/sam3/sam3/agent/agent_core.py\u001b[0m in \u001b[0;36magent_inference\u001b[0;34m(img_path, initial_text_prompt, debug, send_generate_request, call_sam_service, max_generations, output_dir)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mtool_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_call_json_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid JSON in tool call: {tool_call_json_str}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPATH_TO_LATEST_OUTPUT_JSON\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid JSON in tool call: ERROR: Gemini API call failed: 'ProtoType' object has no attribute 'DESCRIPTOR'"
          ]
        }
      ],
      "source": [
        "# prepare input args and run single image inference\n",
        "image = \"/content/sam3/assets/images/test_image.jpg\"\n",
        "prompt = \"the leftmost child wearing blue vest\"\n",
        "# image = os.path.abspath(image) # This is no longer needed as we use the absolute path directly\n",
        "\n",
        "call_sam_service = partial(call_sam_service_orig, sam3_processor=processor)\n",
        "output_image_path = run_single_image_inference(\n",
        "    image, prompt, llm_config, send_generate_request, call_sam_service,\n",
        "    debug=True, output_dir=\"agent_output\"\n",
        ")\n",
        "\n",
        "# display output\n",
        "if output_image_path is not None:\n",
        "    display(Image(filename=output_image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfaf4b97"
      },
      "source": [
        "### Fixing Protobuf and Google Generative AI Installation\n",
        "\n",
        "The previous error `ProtoType' object has no attribute 'DESCRIPTOR'` often indicates an issue with the `protobuf` library or an incompatibility with `google-generativeai`. To resolve this, we will forcefully reinstall and upgrade these packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e3000d2"
      },
      "source": [
        "%%capture\n",
        "# Force reinstall protobuf\n",
        "!pip install --upgrade --force-reinstall protobuf\n",
        "\n",
        "# Force reinstall and upgrade google-generativeai\n",
        "!pip install --upgrade --force-reinstall google-generativeai\n",
        "\n",
        "!pip install -U -q \"google\"\n",
        "!pip install -U -q \"google.genai\"\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "# Please ensure that uploaded files are available in the AI Studio folder or change the working folder.\n",
        "os.chdir(\"/content/drive/MyDrive/Google AI Studio\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "743aba8c"
      },
      "source": [
        "### Fixing Protobuf and Google Generative AI Installation\n",
        "\n",
        "The previous error `ProtoType' object has no attribute 'DESCRIPTOR'` often indicates an issue with the `protobuf` library or an incompatibility with `google-generativeai`. To resolve this, we will forcefully reinstall and upgrade these packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c6903f1"
      },
      "source": [
        "%%capture\n",
        "# Force reinstall protobuf\n",
        "!pip install --upgrade --force-reinstall protobuf\n",
        "\n",
        "# Force reinstall and upgrade google-generativeai\n",
        "!pip install --upgrade --force-reinstall google-generativeai\n",
        "\n",
        "!pip install -U -q \"google\"\n",
        "!pip install -U -q \"google.genai\"\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "# Please ensure that uploaded files are available in the AI Studio folder or change the working folder.\n",
        "os.chdir(\"/content/drive/MyDrive/Google AI Studio\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d04c9fc4"
      },
      "source": [
        "# Check numpy version before torch installation\n",
        "!pip show numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21741566"
      },
      "source": [
        "%%capture\n",
        "!pip install torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e61bd94"
      },
      "source": [
        "# Check numpy version after torch installation\n",
        "!pip show numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef34b0df"
      },
      "source": [
        "import fileinput\n",
        "import os\n",
        "\n",
        "sam3_root_dir = \"/content/sam3\" # Using absolute path\n",
        "\n",
        "pyproject_path = os.path.join(sam3_root_dir, \"pyproject.toml\")\n",
        "temp_file_path = pyproject_path + \".tmp\"\n",
        "\n",
        "# Check if pyproject.toml exists before trying to modify\n",
        "if os.path.exists(pyproject_path):\n",
        "    print(f\"Attempting to modify {pyproject_path} to remove strict numpy version pinning.\")\n",
        "    modified_lines = []\n",
        "    with open(pyproject_path, 'r') as infile:\n",
        "        for line in infile:\n",
        "            if 'numpy==' in line:\n",
        "                # Replace the pinned version, allowing pip to use a compatible version\n",
        "                modified_line = line.replace('numpy==1.26', 'numpy')\n",
        "                modified_lines.append(modified_line)\n",
        "            else:\n",
        "                modified_lines.append(line)\n",
        "\n",
        "    with open(temp_file_path, 'w') as outfile:\n",
        "        outfile.writelines(modified_lines)\n",
        "\n",
        "    os.replace(temp_file_path, pyproject_path)\n",
        "    print(f\"Modified {pyproject_path}.\")\n",
        "else:\n",
        "    print(f\"Warning: {pyproject_path} not found. Skipping pyproject.toml modification.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d84ad9ec"
      },
      "source": [
        "# Check numpy version after pyproject.toml modification\n",
        "!pip show numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e97e56b"
      },
      "source": [
        "%%capture\n",
        "import os\n",
        "\n",
        "# Use absolute path for pip install\n",
        "!pip install -e /content/sam3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee0595a7"
      },
      "source": [
        "# Check numpy version after sam3 installation\n",
        "!pip show numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf3d6ce4"
      },
      "source": [
        "%%capture\n",
        "# For running example notebooks\n",
        "!pip install -e \"/content/sam3[notebooks]\"\n",
        "\n",
        "# For development\n",
        "!pip install -e \"/content/sam3[train,dev]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "253b83ec"
      },
      "source": [
        "# Check numpy version after sam3 extra installations\n",
        "!pip show numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "845fbb66"
      },
      "source": [
        "import torch\n",
        "# turn on tfloat32 for Ampere GPUs\n",
        "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# use bfloat16 for the entire notebook. If your card doesn't support it, try float16 instead\n",
        "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "# inference mode for the whole notebook. Disable if you need gradients\n",
        "torch.inference_mode().__enter__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vUz2sdIUBmT",
        "outputId": "827f9e02-1e38-4090-ae1b-7c196c260f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "# Please ensure that uploaded files are available in the AI Studio folder or change the working folder.\n",
        "os.chdir(\"/content/drive/MyDrive/Google AI Studio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7cgmxgoUBmT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "be59e249-6c09-4634-a9e7-1f06fd233c42",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a58249780a054b65834b14cd4e5293ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_782eb73fbc64434fb7841da99830abc6",
              "IPY_MODEL_599c40e16a2e400a938b98aca2b101a1",
              "IPY_MODEL_342af68877a94b7a9978c4c1259dcba6"
            ],
            "layout": "IPY_MODEL_1427a8e66c2c4634aee8706ef1aa1edd"
          }
        },
        "782eb73fbc64434fb7841da99830abc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86a7545da1ca4deab4d9e2ac5e227607",
            "placeholder": "​",
            "style": "IPY_MODEL_e7fac4fd0c6e4a74aac104b639b123fe",
            "value": "config.json: 100%"
          }
        },
        "599c40e16a2e400a938b98aca2b101a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46eaf90f007f443eb91c3d4161de193b",
            "max": 25843,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_829bef6440dd42f89905ac6f484401a2",
            "value": 25843
          }
        },
        "342af68877a94b7a9978c4c1259dcba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85fe51fe50d4928854f3fd78ebe31d3",
            "placeholder": "​",
            "style": "IPY_MODEL_40fceb5cee4c4901bc0e09b40ffad867",
            "value": " 25.8k/25.8k [00:00&lt;00:00, 2.17MB/s]"
          }
        },
        "1427a8e66c2c4634aee8706ef1aa1edd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86a7545da1ca4deab4d9e2ac5e227607": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7fac4fd0c6e4a74aac104b639b123fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46eaf90f007f443eb91c3d4161de193b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829bef6440dd42f89905ac6f484401a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b85fe51fe50d4928854f3fd78ebe31d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40fceb5cee4c4901bc0e09b40ffad867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88fdf57c6bf2445c9f823eccb50723bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_669c4d70c1214798b1fbf3c9582aba45",
              "IPY_MODEL_0457fc948c944d4fb0aacbdf18538366",
              "IPY_MODEL_72ee877b31214dd99ef3d8f43f1b54b2"
            ],
            "layout": "IPY_MODEL_e823122c6edd448ea4a95bea13e247c6"
          }
        },
        "669c4d70c1214798b1fbf3c9582aba45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eca908d9bb7649be96d0824ca99c3a8f",
            "placeholder": "​",
            "style": "IPY_MODEL_4ccc7efc183d43afb75fe4071cf301ff",
            "value": "sam3.pt: 100%"
          }
        },
        "0457fc948c944d4fb0aacbdf18538366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6980adb692094bdd8673048be5b40841",
            "max": 3450062241,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aea689d0b30f40c887465fc6a077ea3e",
            "value": 3450062241
          }
        },
        "72ee877b31214dd99ef3d8f43f1b54b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f840e83ae743468481cc6ac785e1cfd9",
            "placeholder": "​",
            "style": "IPY_MODEL_90fdb0d6f1f9430a87030789b7a6652b",
            "value": " 3.45G/3.45G [00:34&lt;00:00, 77.2MB/s]"
          }
        },
        "e823122c6edd448ea4a95bea13e247c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eca908d9bb7649be96d0824ca99c3a8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ccc7efc183d43afb75fe4071cf301ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6980adb692094bdd8673048be5b40841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aea689d0b30f40c887465fc6a077ea3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f840e83ae743468481cc6ac785e1cfd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90fdb0d6f1f9430a87030789b7a6652b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}